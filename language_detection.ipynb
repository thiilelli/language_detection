{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "nltk.download('wordnet')\n",
    "nltk.download('sentiwordnet')\n",
    "\n",
    "sub_sets = wn.synsets('love')\n",
    "print(sub_sets)\n",
    "for syn in sub_sets:\n",
    "    print(syn.name(), syn.definition(),syn.examples())\n",
    "love =  swn.senti_synset('love.n.01') \n",
    "print(love.pos_score(), love.neg_score(),love.obj_score()) \n",
    "\n",
    "for syn in wn.synsets('good') :\n",
    "    x= swn.senti_synset(syn.name())\n",
    "    print('positive',x.pos_score(),'negative',x.neg_score(),'neutre',x.obj_score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Discover our current special offers Over 1000 products on promotion Contact  Follow us on Facebook Terms and conditions of sale and legal notices\n",
      "available at the bottom of the page\n",
      "\n",
      "\n",
      " discover our current special offers over 1000 products on promotion contact  follow us on facebook terms and conditions of sale and legal notices\n",
      "available at the bottom of the page\n",
      "\n",
      "\n",
      "[[('discover', 'RB'), ('our', 'PRP$'), ('current', 'JJ'), ('special', 'JJ'), ('offers', 'NNS'), ('over', 'IN'), ('1000', 'CD'), ('products', 'NNS'), ('on', 'IN'), ('promotion', 'NN'), ('contact', 'NN'), ('follow', 'VBP'), ('us', 'PRP'), ('on', 'IN'), ('facebook', 'NN'), ('terms', 'NNS'), ('and', 'CC'), ('conditions', 'NNS'), ('of', 'IN'), ('sale', 'NN'), ('and', 'CC'), ('legal', 'JJ'), ('notices', 'NNS'), ('available', 'JJ'), ('at', 'IN'), ('the', 'DT'), ('bottom', 'NN'), ('of', 'IN'), ('the', 'DT'), ('page', 'NN')]]\n",
      "Lemmatization: [['discover', 'our', 'current', 'special', 'offer', 'over', '1000', 'product', 'on', 'promotion', 'contact', 'follow', 'u', 'on', 'facebook', 'term', 'and', 'condition', 'of', 'sale', 'and', 'legal', 'notice', 'available', 'at', 'the', 'bottom', 'of', 'the', 'page']]\n",
      "Empty DataFrame\n",
      "Columns: [token, lemma, pos]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt') \n",
    "nltk.download('wordnet') \n",
    "text = ''' Discover our current <b>special offers</b>! Over 1000 products on promotion. Contact: <a\n",
    "href=\"mailto:service@sale.com\">service@sale.com</a>. Follow us on <a\n",
    "href=\"https://www.facebook.com\">Facebook</a>! Terms and conditions of sale and legal notices\n",
    "available at the bottom of the page.\n",
    "\n",
    "'''\n",
    "regexp1 = r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+'\n",
    "regexp2 = r'<[^>]+>'\n",
    "regexp3 = r'd\\{10} ' r'd\\+ '\n",
    "cleaned_text = re.sub(regexp1, '',  text)\n",
    "cleaned_text = re.sub(regexp2, '', cleaned_text)\n",
    "\n",
    "#sans ponctuation + miniscule \n",
    "cleaned_text = re.sub( r'[^\\w\\s]', '', cleaned_text)\n",
    "cleaned_text.lower()\n",
    "text_min  = cleaned_text.lower()\n",
    "\n",
    "print(cleaned_text)\n",
    "print(text_min)\n",
    "\n",
    "#tokenization + pos\n",
    "\n",
    "sent = sent_tokenize(text_min)\n",
    "\n",
    "pos_words = [nltk.pos_tag(word_tokenize(s)) for s in sent]\n",
    "print(pos_words)\n",
    "\n",
    " #lemmatization \n",
    "lemma = WordNetLemmatizer()\n",
    "lemmatized_words = []\n",
    "\n",
    "for sentence in pos_words:\n",
    "    lemmatized_sentence = [lemma.lemmatize(word) for word, pos in sentence]\n",
    "    lemmatized_words.append(lemmatized_sentence)\n",
    "\n",
    "print(\"Lemmatization:\",lemmatized_words)\n",
    "\n",
    "df = pd.DataFrame(columns=[\"token\", \"lemma\", \"pos\"])\n",
    "\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_genome_arn = sequence_genome.transcribe()\n",
    "\n",
    "rev = sequence_genome.reverse_complement()\n",
    "def linear_search(sequence,pattern):\n",
    "\n",
    "    indices =[]\n",
    "    for i in range(len(sequence)- len(pattern) +1):\n",
    "        if sequence[i:i+len(pattern)]== pattern :\n",
    "\n",
    "            indices.append(i)\n",
    "    return indices\n",
    "pattern = 'AGCGCATGC'\n",
    "indices = linear_search(sequence_genome,pattern)\n",
    "print(indices)\n",
    "\n",
    "def trouver_motif(adn, motif):\n",
    "    positions = []\n",
    "    for i in range(len(adn) - len(motif) + 1):\n",
    "        if adn[i:i+len(motif)].upper() == motif.upper():\n",
    "            positions.append(i+1) \n",
    "    return positions\n",
    "\n",
    "positions_motif = trouver_motif(\"AGCTTTTCATTCTGACT\", \"CGT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.Data import CodonTable\n",
    "\n",
    "#file = r\"C:\\Users\\DELL\\Downloads\\Escherichia_coli.fna\"\n",
    "codon = ('TAA', 'TAG', 'TGA')\n",
    "sequence_genome = SeqIO.read( r\"C:/users/DELL/Downloads/Escherichia_coli.fna\", \"fasta\").seq\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "sequence_proteine = [] \n",
    "orfs = ['ATGTTTTCATTCTGAGCTGA']\n",
    "arn_messager = sequence_genome.transcribe() \n",
    "proteine =  arn_messager.translate()\n",
    "sequence_proteine.append(proteine)\n",
    "\n",
    "print(sequence_proteine)\n",
    "\n",
    "for i , proteine in enumerate(sequence_proteine) :\n",
    "    print(f'proteine', {i+1})\n",
    "    print(f' longeure proteine', {len(proteine)})\n",
    "    print('composition en acides aminees:')\n",
    "    for aa in set(proteine):\n",
    "       print({proteine(aa)},{sequence_proteine.count(aa)}/{len(proteine)*100},'frequence des acides aminees ')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          clean_text  category\n",
      "0  when modi promised “minimum government maximum...      -1.0\n",
      "1  talk all the nonsense and continue all the dra...       0.0\n",
      "2  what did just say vote for modi  welcome bjp t...       1.0\n",
      "3  asking his supporters prefix chowkidar their n...       1.0\n",
      "4  answer who among these the most powerful world...       1.0\n",
      "Index(['clean_text', 'category'], dtype='object')\n",
      "when modi promised “minimum government maximum governance” expected him begin the difficult job reforming the state why does take years get justice state should and not business and should exit psus and temples\n",
      "[['when', 'when', 'WRB'], ['modi', 'modi', 'NN'], ['promised', 'promise', 'VBN'], ['minimum', 'minimum', 'NN'], ['government', 'government', 'NN'], ['maximum', 'maximum', 'NN'], ['governance', 'governance', 'NN'], ['expected', 'expect', 'VBN'], ['him', 'him', 'PRP'], ['begin', 'begin', 'NN'], ['the', 'the', 'DT'], ['difficult', 'difficult', 'JJ'], ['job', 'job', 'NN'], ['reforming', 'reform', 'VBG'], ['the', 'the', 'DT'], ['state', 'state', 'NN'], ['why', 'why', 'WRB'], ['does', 'do', 'VBZ'], ['take', 'take', 'VB'], ['years', 'year', 'NNS'], ['get', 'get', 'VB'], ['justice', 'justice', 'NN'], ['state', 'state', 'NN'], ['should', 'should', 'MD'], ['and', 'and', 'CC'], ['not', 'not', 'RB'], ['business', 'business', 'NN'], ['and', 'and', 'CC'], ['should', 'should', 'MD'], ['exit', 'exit', 'NN'], ['psus', 'psus', 'NN'], ['and', 'and', 'CC'], ['temples', 'temple', 'NNS'], ['when', 'modi', 'promised', '“', 'minimum', 'government', 'maximum', 'governance', '”', 'expected', 'him', 'begin', 'the', 'difficult', 'job', 'reforming', 'the', 'state', 'why', 'does', 'take', 'years', 'get', 'justice', 'state', 'should', 'and', 'not', 'business', 'and', 'should', 'exit', 'psus', 'and', 'temples'], ['talk', 'talk', 'NN'], ['all', 'all', 'DT'], ['the', 'the', 'DT'], ['nonsense', 'nonsense', 'NN'], ['and', 'and', 'CC'], ['continue', 'continue', 'NN'], ['all', 'all', 'DT'], ['the', 'the', 'DT'], ['drama', 'drama', 'NN'], ['will', 'will', 'MD'], ['vote', 'vote', 'NN'], ['for', 'for', 'IN'], ['modi', 'modi', 'NN'], ['talk', 'all', 'the', 'nonsense', 'and', 'continue', 'all', 'the', 'drama', 'will', 'vote', 'for', 'modi'], ['what', 'what', 'WP'], ['did', 'do', 'VBD'], ['just', 'just', 'RB'], ['say', 'say', 'VB'], ['vote', 'vote', 'NN'], ['for', 'for', 'IN'], ['modi', 'modi', 'NN'], ['welcome', 'welcome', 'NN'], ['bjp', 'bjp', 'NN'], ['told', 'told', 'NN'], ['you', 'you', 'PRP'], ['rahul', 'rahul', 'NN'], ['the', 'the', 'DT'], ['main', 'main', 'JJ'], ['campaigner', 'campaigner', 'NN'], ['for', 'for', 'IN'], ['modi', 'modi', 'NN'], ['think', 'think', 'NN'], ['modi', 'modi', 'NN'], ['should', 'should', 'MD'], ['just', 'just', 'RB'], ['relax', 'relax', 'NN'], ['what', 'did', 'just', 'say', 'vote', 'for', 'modi', 'welcome', 'bjp', 'told', 'you', 'rahul', 'the', 'main', 'campaigner', 'for', 'modi', 'think', 'modi', 'should', 'just', 'relax'], ['asking', 'ask', 'VBG'], ['his', 'his', 'PRP$'], ['supporters', 'supporter', 'NNS'], ['prefix', 'prefix', 'NN'], ['chowkidar', 'chowkidar', 'NN'], ['their', 'their', 'PRP$'], ['names', 'name', 'NNS'], ['modi', 'modi', 'NN'], ['did', 'do', 'VBD'], ['great', 'great', 'JJ'], ['service', 'service', 'NN'], ['now', 'now', 'RB'], ['there', 'there', 'RB'], ['confusion', 'confusion', 'NN'], ['what', 'what', 'WP'], ['read', 'read', 'NN'], ['what', 'what', 'WP'], ['not', 'not', 'RB'], ['now', 'now', 'RB'], ['crustal', 'crustal', 'NN'], ['clear', 'clear', 'JJ'], ['what', 'what', 'WP'], ['will', 'will', 'MD'], ['crass', 'crass', 'NN'], ['filthy', 'filthy', 'NN'], ['nonsensical', 'nonsensical', 'JJ'], ['see', 'see', 'VB'], ['how', 'how', 'WRB'], ['most', 'most', 'JJS'], ['abuses', 'abuse', 'NNS'], ['are', 'be', 'VBP'], ['coming', 'come', 'VBG'], ['from', 'from', 'IN'], ['chowkidars', 'chowkidars', 'NNS'], ['asking', 'his', 'supporters', 'prefix', 'chowkidar', 'their', 'names', 'modi', 'did', 'great', 'service', 'now', 'there', 'confusion', 'what', 'read', 'what', 'not', 'now', 'crustal', 'clear', 'what', 'will', 'crass', 'filthy', 'nonsensical', 'see', 'how', 'most', 'abuses', 'are', 'coming', 'from', 'chowkidars'], ['answer', 'answer', 'NN'], ['who', 'who', 'WP'], ['among', 'among', 'IN'], ['these', 'these', 'DT'], ['the', 'the', 'DT'], ['most', 'most', 'JJS'], ['powerful', 'powerful', 'JJ'], ['world', 'world', 'NN'], ['leader', 'leader', 'NN'], ['today', 'today', 'NN'], ['trump', 'trump', 'NN'], ['putin', 'putin', 'NN'], ['modi', 'modi', 'NN'], ['may', 'may', 'MD'], ['answer', 'who', 'among', 'these', 'the', 'most', 'powerful', 'world', 'leader', 'today', 'trump', 'putin', 'modi', 'may'], ['kiya', 'kiya', 'NN'], ['tho', 'tho', 'NN'], ['refresh', 'refresh', 'NN'], ['maarkefir', 'maarkefir', 'NN'], ['comment', 'comment', 'NN'], ['karo', 'karo', 'NN'], ['kiya', 'tho', 'refresh', 'maarkefir', 'comment', 'karo'], ['surat', 'surat', 'NN'], ['women', 'woman', 'NNS'], ['perform', 'perform', 'NN'], ['yagna', 'yagna', 'NN'], ['seeks', 'seek', 'NN'], ['divine', 'divine', 'NN'], ['grace', 'grace', 'NN'], ['for', 'for', 'IN'], ['narendra', 'narendra', 'NN'], ['modi', 'modi', 'NN'], ['become', 'become', 'NN'], ['again', 'again', 'RB'], ['surat', 'women', 'perform', 'yagna', 'seeks', 'divine', 'grace', 'for', 'narendra', 'modi', 'become', 'again'], ['this', 'this', 'DT'], ['comes', 'come', 'VBZ'], ['from', 'from', 'IN'], ['cabinet', 'cabinet', 'NN'], ['which', 'which', 'WDT'], ['has', 'have', 'VBZ'], ['scholars', 'scholar', 'NNS'], ['like', 'like', 'IN'], ['modi', 'modi', 'NN'], ['smriti', 'smriti', 'NN'], ['and', 'and', 'CC'], ['hema', 'hema', 'NN'], ['time', 'time', 'NN'], ['introspect', 'introspect', 'NN'], ['this', 'comes', 'from', 'cabinet', 'which', 'has', 'scholars', 'like', 'modi', 'smriti', 'and', 'hema', 'time', 'introspect'], ['with', 'with', 'IN'], ['upcoming', 'upcoming', 'VBG'], ['election', 'election', 'NN'], ['india', 'india', 'NN'], ['saga', 'saga', 'NN'], ['going', 'go', 'VBG'], ['important', 'important', 'JJ'], ['pair', 'pair', 'NN'], ['look', 'look', 'NN'], ['current', 'current', 'JJ'], ['modi', 'modi', 'NN'], ['leads', 'lead', 'NNS'], ['govt', 'govt', 'NN'], ['elected', 'elect', 'VBN'], ['with', 'with', 'IN'], ['deal', 'deal', 'NN'], ['brexit', 'brexit', 'NN'], ['combination', 'combination', 'NN'], ['this', 'this', 'DT'], ['weekly', 'weekly', 'JJ'], ['looks', 'look', 'NNS'], ['juicy', 'juicy', 'NN'], ['bears', 'bear', 'NNS'], ['imho', 'imho', 'NN'], ['with', 'upcoming', 'election', 'india', 'saga', 'going', 'important', 'pair', 'look', 'current', 'modi', 'leads', 'govt', 'elected', 'with', 'deal', 'brexit', 'combination', 'this', 'weekly', 'looks', 'juicy', 'bears', 'imho'], ['gandhi', 'gandhi', 'NN'], ['was', 'be', 'VBD'], ['gay', 'gay', 'NN'], ['does', 'do', 'VBZ'], ['modi', 'modi', 'NN'], ['gandhi', 'was', 'gay', 'does', 'modi'], ['things', 'thing', 'NNS'], ['like', 'like', 'IN'], ['demonetisation', 'demonetisation', 'NN'], ['gst', 'gst', 'NN'], ['goods', 'good', 'NNS'], ['and', 'and', 'CC'], ['services', 'service', 'NNS'], ['upper', 'upper', 'JJ'], ['castes', 'caste', 'NNS'], ['would', 'would', 'MD'], ['sort', 'sort', 'NN'], ['either', 'either', 'DT'], ['view', 'view', 'NN'], ['favourably', 'favourably', 'RB'], ['say', 'say', 'VB'], ['that', 'that', 'IN'], ['need', 'need', 'NN'], ['give', 'give', 'VB'], ['this', 'this', 'DT'], ['more', 'more', 'RBR'], ['time', 'time', 'NN'], ['other', 'other', 'JJ'], ['castes', 'caste', 'NNS'], ['like', 'like', 'IN'], ['dalits', 'dalits', 'NNS'], ['the', 'the', 'DT'], ['muslims', 'muslim', 'NNS'], ['were', 'be', 'VBD'], ['more', 'more', 'RBR'], ['against', 'against', 'IN'], ['because', 'because', 'IN'], ['that', 'that', 'IN'], ['just', 'just', 'RB'], ['not', 'not', 'RB'], ['modi', 'modi', 'NN'], ['constituency2', 'constituency2', 'NN'], ['things', 'like', 'demonetisation', 'gst', 'goods', 'and', 'services', 'tax…the', 'upper', 'castes', 'would', 'sort', 'either', 'view', 'favourably', 'say', 'that', 'need', 'give', 'this', 'more', 'time', 'other', 'castes', 'like', 'dalits', 'the', 'muslims', 'were', 'more', 'against', 'because', 'that', '’', 'just', 'not', 'modi', '’', 'constituency2']]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "3 columns passed, passed data had 39 columns",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\internals\\construction.py:939\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[1;34m(content, columns, dtype)\u001b[0m\n\u001b[0;32m    938\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 939\u001b[0m     columns \u001b[39m=\u001b[39m _validate_or_indexify_columns(contents, columns)\n\u001b[0;32m    940\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    941\u001b[0m     \u001b[39m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\internals\\construction.py:986\u001b[0m, in \u001b[0;36m_validate_or_indexify_columns\u001b[1;34m(content, columns)\u001b[0m\n\u001b[0;32m    984\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_mi_list \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(columns) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(content):  \u001b[39m# pragma: no cover\u001b[39;00m\n\u001b[0;32m    985\u001b[0m     \u001b[39m# caller's responsibility to check for this...\u001b[39;00m\n\u001b[1;32m--> 986\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[0;32m    987\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(columns)\u001b[39m}\u001b[39;00m\u001b[39m columns passed, passed data had \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    988\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(content)\u001b[39m}\u001b[39;00m\u001b[39m columns\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    989\u001b[0m     )\n\u001b[0;32m    990\u001b[0m \u001b[39mif\u001b[39;00m is_mi_list:\n\u001b[0;32m    991\u001b[0m     \u001b[39m# check if nested list column, length of each sub-list should be equal\u001b[39;00m\n",
      "\u001b[1;31mAssertionError\u001b[0m: 3 columns passed, passed data had 39 columns",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mUntitled-1.ipynb Cell 7\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W6sdW50aXRsZWQ%3D?line=35'>36</a>\u001b[0m \u001b[39mprint\u001b[39m(data) \n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W6sdW50aXRsZWQ%3D?line=37'>38</a>\u001b[0m data\u001b[39m.\u001b[39mappend([\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m])  \n\u001b[1;32m---> <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W6sdW50aXRsZWQ%3D?line=38'>39</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame(data, columns\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mtoken\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mlemma\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mpos\u001b[39;49m\u001b[39m'\u001b[39;49m] )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\frame.py:806\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    804\u001b[0m     \u001b[39mif\u001b[39;00m columns \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    805\u001b[0m         columns \u001b[39m=\u001b[39m ensure_index(columns)\n\u001b[1;32m--> 806\u001b[0m     arrays, columns, index \u001b[39m=\u001b[39m nested_data_to_arrays(\n\u001b[0;32m    807\u001b[0m         \u001b[39m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[39;49;00m\n\u001b[0;32m    808\u001b[0m         \u001b[39m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[39;49;00m\n\u001b[0;32m    809\u001b[0m         data,\n\u001b[0;32m    810\u001b[0m         columns,\n\u001b[0;32m    811\u001b[0m         index,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    812\u001b[0m         dtype,\n\u001b[0;32m    813\u001b[0m     )\n\u001b[0;32m    814\u001b[0m     mgr \u001b[39m=\u001b[39m arrays_to_mgr(\n\u001b[0;32m    815\u001b[0m         arrays,\n\u001b[0;32m    816\u001b[0m         columns,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    819\u001b[0m         typ\u001b[39m=\u001b[39mmanager,\n\u001b[0;32m    820\u001b[0m     )\n\u001b[0;32m    821\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\internals\\construction.py:520\u001b[0m, in \u001b[0;36mnested_data_to_arrays\u001b[1;34m(data, columns, index, dtype)\u001b[0m\n\u001b[0;32m    517\u001b[0m \u001b[39mif\u001b[39;00m is_named_tuple(data[\u001b[39m0\u001b[39m]) \u001b[39mand\u001b[39;00m columns \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    518\u001b[0m     columns \u001b[39m=\u001b[39m ensure_index(data[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39m_fields)\n\u001b[1;32m--> 520\u001b[0m arrays, columns \u001b[39m=\u001b[39m to_arrays(data, columns, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[0;32m    521\u001b[0m columns \u001b[39m=\u001b[39m ensure_index(columns)\n\u001b[0;32m    523\u001b[0m \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\internals\\construction.py:845\u001b[0m, in \u001b[0;36mto_arrays\u001b[1;34m(data, columns, dtype)\u001b[0m\n\u001b[0;32m    842\u001b[0m     data \u001b[39m=\u001b[39m [\u001b[39mtuple\u001b[39m(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m data]\n\u001b[0;32m    843\u001b[0m     arr \u001b[39m=\u001b[39m _list_to_arrays(data)\n\u001b[1;32m--> 845\u001b[0m content, columns \u001b[39m=\u001b[39m _finalize_columns_and_data(arr, columns, dtype)\n\u001b[0;32m    846\u001b[0m \u001b[39mreturn\u001b[39;00m content, columns\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\internals\\construction.py:942\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[1;34m(content, columns, dtype)\u001b[0m\n\u001b[0;32m    939\u001b[0m     columns \u001b[39m=\u001b[39m _validate_or_indexify_columns(contents, columns)\n\u001b[0;32m    940\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    941\u001b[0m     \u001b[39m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n\u001b[1;32m--> 942\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(err) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m    944\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(contents) \u001b[39mand\u001b[39;00m contents[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mobject_:\n\u001b[0;32m    945\u001b[0m     contents \u001b[39m=\u001b[39m convert_object_array(contents, dtype\u001b[39m=\u001b[39mdtype)\n",
      "\u001b[1;31mValueError\u001b[0m: 3 columns passed, passed data had 39 columns"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk \n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt') \n",
    "nltk.download('wordnet') \n",
    "\n",
    "\n",
    "corpus = pd.read_csv(\"C:/Users/DELL/Downloads/archive (1)/Twitter_Data.csv\")\n",
    "\n",
    "print(corpus.head())\n",
    "print(corpus.columns)\n",
    "  \n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "POS_TAG_MAP = {\n",
    "    'N': 'n',  \n",
    "    'V': 'v',  \n",
    "    'R': 'r',  \n",
    "    'J': 'a'   \n",
    "}\n",
    "def get_wordnet_pos(tag):\n",
    "    return POS_TAG_MAP.get(tag[0], 'n') \n",
    "\n",
    "data =[]\n",
    "print(corpus.loc[0,'clean_text'])\n",
    "for text in corpus.loc[:10,'clean_text'] :\n",
    "   tokens = word_tokenize(text)\n",
    "   for token in tokens :\n",
    "    if token.isalnum():  \n",
    "            pos = nltk.pos_tag([token])[0][1]\n",
    "            lemma = lemmatizer.lemmatize(token, get_wordnet_pos(pos))\n",
    "            data.append([token, lemma, pos])\n",
    "   data.append(tokens)\n",
    "print(data) \n",
    "            \n",
    "data.append([' ', ' ', ' '])  \n",
    "df = pd.DataFrame(data, columns=['token', 'lemma','pos'] )\n",
    "   \n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          clean_text  category\n",
      "0  when modi promised “minimum government maximum...      -1.0\n",
      "1  talk all the nonsense and continue all the dra...       0.0\n",
      "2  what did just say vote for modi  welcome bjp t...       1.0\n",
      "3  asking his supporters prefix chowkidar their n...       1.0\n",
      "4  answer who among these the most powerful world...       1.0\n",
      "Index(['clean_text', 'category'], dtype='object')\n",
      "             token          lemma  pos\n",
      "0             when           when  WRB\n",
      "1             modi           modi   NN\n",
      "2         promised        promise  VBN\n",
      "3          minimum        minimum   NN\n",
      "4       government     government   NN\n",
      "..             ...            ...  ...\n",
      "209           just           just   RB\n",
      "210            not            not   RB\n",
      "211           modi           modi   NN\n",
      "212  constituency2  constituency2   NN\n",
      "213                                   \n",
      "\n",
      "[214 rows x 3 columns]\n",
      "[['when', 'modi', 'promised', 'minimum', 'government', 'maximum', 'governance', 'expected', 'him', 'begin', 'the', 'difficult', 'job', 'reforming', 'the', 'state', 'why', 'does', 'take', 'years', 'get', 'justice', 'state', 'should', 'and', 'not', 'business', 'and', 'should', 'exit', 'psus', 'and', 'temples'], ['talk', 'all', 'the', 'nonsense', 'and', 'continue', 'all', 'the', 'drama', 'will', 'vote', 'for', 'modi'], ['what', 'did', 'just', 'say', 'vote', 'for', 'modi', 'welcome', 'bjp', 'told', 'you', 'rahul', 'the', 'main', 'campaigner', 'for', 'modi', 'think', 'modi', 'should', 'just', 'relax'], ['asking', 'his', 'supporters', 'prefix', 'chowkidar', 'their', 'names', 'modi', 'did', 'great', 'service', 'now', 'there', 'confusion', 'what', 'read', 'what', 'not', 'now', 'crustal', 'clear', 'what', 'will', 'crass', 'filthy', 'nonsensical', 'see', 'how', 'most', 'abuses', 'are', 'coming', 'from', 'chowkidars'], ['answer', 'who', 'among', 'these', 'the', 'most', 'powerful', 'world', 'leader', 'today', 'trump', 'putin', 'modi', 'may'], ['kiya', 'tho', 'refresh', 'maarkefir', 'comment', 'karo'], ['surat', 'women', 'perform', 'yagna', 'seeks', 'divine', 'grace', 'for', 'narendra', 'modi', 'become', 'again'], ['this', 'comes', 'from', 'cabinet', 'which', 'has', 'scholars', 'like', 'modi', 'smriti', 'and', 'hema', 'time', 'introspect'], ['with', 'upcoming', 'election', 'india', 'saga', 'going', 'important', 'pair', 'look', 'current', 'modi', 'leads', 'govt', 'elected', 'with', 'deal', 'brexit', 'combination', 'this', 'weekly', 'looks', 'juicy', 'bears', 'imho'], ['gandhi', 'was', 'gay', 'does', 'modi'], ['things', 'like', 'demonetisation', 'gst', 'goods', 'and', 'services', 'upper', 'castes', 'would', 'sort', 'either', 'view', 'favourably', 'say', 'that', 'need', 'give', 'this', 'more', 'time', 'other', 'castes', 'like', 'dalits', 'the', 'muslims', 'were', 'more', 'against', 'because', 'that', 'just', 'not', 'modi', 'constituency2']]\n",
      "['when modi promised minimum government maximum governance expected him begin the difficult job reforming the state why does take years get justice state should and not business and should exit psus and temples', 'talk all the nonsense and continue all the drama will vote for modi', 'what did just say vote for modi welcome bjp told you rahul the main campaigner for modi think modi should just relax', 'asking his supporters prefix chowkidar their names modi did great service now there confusion what read what not now crustal clear what will crass filthy nonsensical see how most abuses are coming from chowkidars', 'answer who among these the most powerful world leader today trump putin modi may', 'kiya tho refresh maarkefir comment karo', 'surat women perform yagna seeks divine grace for narendra modi become again', 'this comes from cabinet which has scholars like modi smriti and hema time introspect', 'with upcoming election india saga going important pair look current modi leads govt elected with deal brexit combination this weekly looks juicy bears imho', 'gandhi was gay does modi', 'things like demonetisation gst goods and services upper castes would sort either view favourably say that need give this more time other castes like dalits the muslims were more against because that just not modi constituency2']\n",
      "Matrice TF-IDF : [[0.         0.         0.         ... 0.         0.16926619 0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.22606068]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.15968814 ... 0.         0.         0.        ]]\n",
      "Vecteurs de document:\n",
      "[[ 0.04242325  0.01730537  0.01561832  0.17472076 -0.10797876  0.00560653\n",
      "   0.07060432 -0.06086349  0.0030098   0.08409286 -0.01301622 -0.10124016\n",
      "  -0.06254196  0.03863716 -0.10003376  0.06382513  0.03586483  0.06685638\n",
      "   0.00152493 -0.03298289 -0.0135864  -0.04882812  0.05595398 -0.04712129\n",
      "  -0.00271034  0.00051308 -0.1295042   0.03420258  0.02428293  0.03528023\n",
      "  -0.07813454  0.04253292 -0.02271032 -0.00136483  0.00792217  0.00162005\n",
      "   0.08184814 -0.05008316  0.01450157  0.05117488  0.06201172  0.00583285\n",
      "   0.15875626 -0.01125884 -0.01226783 -0.1104579  -0.03841639  0.02566034\n",
      "  -0.0408783  -0.00508547  0.02995205 -0.00947762 -0.0294323  -0.05191803\n",
      "  -0.02427292  0.0135231  -0.04020119 -0.03138351  0.01362109 -0.06132174\n",
      "  -0.02898788  0.04867435 -0.09476566 -0.10313928 -0.01455498 -0.03547692\n",
      "  -0.01072121  0.13095474 -0.057868    0.06480217  0.01749229  0.05550385\n",
      "   0.09607697  0.02095795 -0.07441521 -0.12589264  0.06549835  0.08241487\n",
      "   0.06011391  0.05992508 -0.03369939  0.02064323  0.03643322 -0.00053072\n",
      "  -0.0597043  -0.08146477 -0.05069923  0.08149624  0.01918411 -0.00206757\n",
      "   0.00413048  0.03989792 -0.11827826 -0.08733177  0.0139637  -0.09036112\n",
      "   0.04839802  0.01838446  0.06270027  0.00734472 -0.01035076 -0.0270443\n",
      "   0.0315876   0.00557327 -0.03562593 -0.05705667 -0.05935478  0.02087593\n",
      "   0.08688545 -0.03928089 -0.06197929 -0.03109074 -0.00704811  0.03019333\n",
      "   0.05642509  0.03721809  0.0503664  -0.07916451  0.07092559  0.06692696\n",
      "  -0.09350085  0.12672043 -0.11297512  0.04894161 -0.08811569  0.00405979\n",
      "  -0.01433754 -0.06704521  0.00338602 -0.03997731 -0.02994442 -0.06640244\n",
      "  -0.06583691 -0.02172279 -0.0153923  -0.10706282 -0.00443745 -0.0092206\n",
      "   0.01709104  0.02550316  0.0684855  -0.12454987  0.06233168  0.01528263\n",
      "   0.03952217 -0.00686932 -0.01992464 -0.06545639 -0.0307169  -0.01094055\n",
      "   0.08099079  0.01736927 -0.14823627  0.01733589 -0.00484753 -0.03814888\n",
      "  -0.08339882 -0.0835495  -0.04540634 -0.05943108 -0.0142107  -0.01002216\n",
      "  -0.03055954  0.03636646  0.00142002 -0.10427761  0.01988411 -0.00246751\n",
      "  -0.01771545  0.01438522 -0.20799255 -0.04786205 -0.07405186 -0.06976509\n",
      "   0.02631426 -0.11094791  0.07197952 -0.10492134 -0.03226662 -0.01495409\n",
      "  -0.07161951 -0.03383255 -0.03077382  0.01064301  0.01771402 -0.00890732\n",
      "   0.01548988  0.03027725  0.09911156  0.05821753  0.01724672 -0.03355265\n",
      "  -0.03663635 -0.01150131  0.03799057 -0.03695118  0.0033226   0.03027248\n",
      "  -0.01717162 -0.13391113 -0.01300907  0.03488532 -0.03511244 -0.03225756\n",
      "  -0.02255583 -0.02127266 -0.02512026 -0.0891757   0.01106262 -0.01330757\n",
      "   0.00577641  0.01048756  0.01312733  0.03163147 -0.10306549  0.01130438\n",
      "   0.07932174  0.08604813 -0.09105057 -0.05551529  0.01973689  0.00619125\n",
      "  -0.01897478 -0.0199213   0.04284859 -0.07394218  0.08877277  0.05057144\n",
      "   0.03736639  0.08035469  0.00124025 -0.01777935  0.02622056  0.03599976\n",
      "   0.02392411 -0.00269413  0.01595592 -0.06119156  0.12700891  0.0344708\n",
      "   0.10195494  0.02307224  0.06768346 -0.06853831 -0.00610638  0.01897335\n",
      "   0.00986862  0.07183933 -0.10290408  0.01655579  0.01383734  0.04242086\n",
      "   0.06903291  0.05386639  0.06809139 -0.07471275  0.01634938  0.03162861\n",
      "  -0.00142384 -0.08109188 -0.00989676  0.00167656 -0.03735352  0.04573727\n",
      "  -0.04194832  0.10096741  0.00498784 -0.01007462 -0.03546906 -0.00882936\n",
      "   0.00642395  0.08057976  0.12800789  0.06335855  0.09653664 -0.0747242\n",
      "  -0.05001402 -0.13512993 -0.02511835  0.02017975  0.00090408 -0.00961828\n",
      "  -0.02515578  0.07045221 -0.01273537 -0.00922084 -0.06449318 -0.0145998\n",
      "  -0.01164627  0.03151751 -0.07262278  0.05157185 -0.04724789  0.01832294\n",
      "  -0.03893924  0.03650486  0.0331974  -0.02739191  0.04546452 -0.03334188]]\n",
      "                                     phrase_tokenise  \\\n",
      "0  [[when, modi, promised, minimum, government, m...   \n",
      "\n",
      "                                tfidf_representation  \\\n",
      "0  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.34113265717172336...   \n",
      "\n",
      "                             word2vec_representation  \n",
      "0  [[0.04242325, 0.017305374, 0.015618324, 0.1747...  \n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "\n",
    "#nltk.download('punkt') \n",
    "#nltk.download('wordnet') \n",
    "\n",
    "corpus = pd.read_csv(\"C:/Users/DELL/Downloads/archive (1)/Twitter_Data.csv\")\n",
    "model = api.load(\"word2vec-google-news-300\")\n",
    "word_vectors = model\n",
    "\n",
    "print(corpus.head())\n",
    "print(corpus.columns)\n",
    "data = []\n",
    "data2 = []\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "POS_TAG_MAP = {\n",
    "    'N': 'n',  \n",
    "    'V': 'v',  \n",
    "    'R': 'r',  \n",
    "    'J': 'a'   \n",
    "}\n",
    "def get_wordnet_pos(tag):\n",
    "    return POS_TAG_MAP.get(tag[0], 'n') \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "documents = []\n",
    "\n",
    "for text in corpus.loc[:5, 'clean_text']:\n",
    "    tokens = word_tokenize(text)\n",
    "    phrase_tokenise = []\n",
    "    for token in tokens:\n",
    "        if token.isalnum():  \n",
    "            pos = nltk.pos_tag([token])[0][1]\n",
    "            lemma = lemmatizer.lemmatize(token, get_wordnet_pos(pos))\n",
    "            data.append([token, lemma, pos])\n",
    "            phrase_tokenise.append(token)\n",
    "    documents.append(phrase_tokenise)\n",
    "\n",
    "data.append([' ', ' ', ' '])  \n",
    "\n",
    "df = pd.DataFrame(data, columns=['token', 'lemma', 'pos'])\n",
    "print(df)\n",
    "print(documents)\n",
    "\n",
    "\n",
    "corpus_document = [' '.join(sentence) for sentence in documents]\n",
    "\n",
    "\n",
    "print(corpus_document)\n",
    "\n",
    "#vectorisation avec tf-idf\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus_document)\n",
    "\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "tfidf_representation = tfidf_matrix.toarray()\n",
    "\n",
    "\n",
    "#print(\"Matrice TF-IDF :\",tfidf_representation)\n",
    "\n",
    "#vectorisation avec word2vec\n",
    "\n",
    "# Calculer la moyenne des vecteurs des mots présents dans le modèle pour chaque document\n",
    "vectors = []\n",
    "doc_vector = np.mean([model[word] for word in phrase_tokenise if word in model], axis=0)\n",
    "vectors.append(doc_vector)\n",
    "\n",
    "# Convertir la liste de vecteurs en un tableau numpy\n",
    "word2vec_representation = np.array(vectors)\n",
    "\n",
    "# Afficher les vecteurs de document\n",
    "print('Vecteurs de document:')\n",
    "print(word2vec_representation)\n",
    "\n",
    "data2.append([documents, tfidf_representation, word2vec_representation])\n",
    "\n",
    "df2 = pd.DataFrame(data2, columns=['phrase_tokenise', 'tfidf_representation', 'word2vec_representation'])\n",
    "print(df2)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mUntitled-1.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X11sdW50aXRsZWQ%3D?line=10'>11</a>\u001b[0m nltk\u001b[39m.\u001b[39mdownload(\u001b[39m'\u001b[39m\u001b[39mpunkt\u001b[39m\u001b[39m'\u001b[39m) \n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X11sdW50aXRsZWQ%3D?line=12'>13</a>\u001b[0m corpus \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m\"\u001b[39m\u001b[39mC:/Users/DELL/Downloads/archive (1)/Twitter_Data.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X11sdW50aXRsZWQ%3D?line=15'>16</a>\u001b[0m model \u001b[39m=\u001b[39m api\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39mword2vec-google-news-300\u001b[39;49m\u001b[39m\"\u001b[39;49m)  \n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X11sdW50aXRsZWQ%3D?line=16'>17</a>\u001b[0m word_vectors \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mwv \n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X11sdW50aXRsZWQ%3D?line=17'>18</a>\u001b[0m documents \u001b[39m=\u001b[39m []  \n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\gensim\\downloader.py:503\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, return_path)\u001b[0m\n\u001b[0;32m    501\u001b[0m sys\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39minsert(\u001b[39m0\u001b[39m, BASE_DIR)\n\u001b[0;32m    502\u001b[0m module \u001b[39m=\u001b[39m \u001b[39m__import__\u001b[39m(name)\n\u001b[1;32m--> 503\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39;49mload_data()\n",
      "File \u001b[1;32m~/gensim-data\\word2vec-google-news-300\\__init__.py:8\u001b[0m, in \u001b[0;36mload_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_data\u001b[39m():\n\u001b[0;32m      7\u001b[0m     path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(base_dir, \u001b[39m'\u001b[39m\u001b[39mword2vec-google-news-300\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mword2vec-google-news-300.gz\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m     model \u001b[39m=\u001b[39m KeyedVectors\u001b[39m.\u001b[39;49mload_word2vec_format(path, binary\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m      9\u001b[0m     \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\gensim\\models\\keyedvectors.py:1719\u001b[0m, in \u001b[0;36mKeyedVectors.load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[0;32m   1672\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m   1673\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_word2vec_format\u001b[39m(\n\u001b[0;32m   1674\u001b[0m         \u001b[39mcls\u001b[39m, fname, fvocab\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, binary\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf8\u001b[39m\u001b[39m'\u001b[39m, unicode_errors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mstrict\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   1675\u001b[0m         limit\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, datatype\u001b[39m=\u001b[39mREAL, no_header\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m   1676\u001b[0m     ):\n\u001b[0;32m   1677\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Load KeyedVectors from a file produced by the original C word2vec-tool format.\u001b[39;00m\n\u001b[0;32m   1678\u001b[0m \n\u001b[0;32m   1679\u001b[0m \u001b[39m    Warnings\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1717\u001b[0m \n\u001b[0;32m   1718\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1719\u001b[0m     \u001b[39mreturn\u001b[39;00m _load_word2vec_format(\n\u001b[0;32m   1720\u001b[0m         \u001b[39mcls\u001b[39;49m, fname, fvocab\u001b[39m=\u001b[39;49mfvocab, binary\u001b[39m=\u001b[39;49mbinary, encoding\u001b[39m=\u001b[39;49mencoding, unicode_errors\u001b[39m=\u001b[39;49municode_errors,\n\u001b[0;32m   1721\u001b[0m         limit\u001b[39m=\u001b[39;49mlimit, datatype\u001b[39m=\u001b[39;49mdatatype, no_header\u001b[39m=\u001b[39;49mno_header,\n\u001b[0;32m   1722\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\gensim\\models\\keyedvectors.py:2065\u001b[0m, in \u001b[0;36m_load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[0;32m   2062\u001b[0m kv \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(vector_size, vocab_size, dtype\u001b[39m=\u001b[39mdatatype)\n\u001b[0;32m   2064\u001b[0m \u001b[39mif\u001b[39;00m binary:\n\u001b[1;32m-> 2065\u001b[0m     _word2vec_read_binary(\n\u001b[0;32m   2066\u001b[0m         fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, binary_chunk_size, encoding\n\u001b[0;32m   2067\u001b[0m     )\n\u001b[0;32m   2068\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2069\u001b[0m     _word2vec_read_text(fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, encoding)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\gensim\\models\\keyedvectors.py:1958\u001b[0m, in \u001b[0;36m_word2vec_read_binary\u001b[1;34m(fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, binary_chunk_size, encoding)\u001b[0m\n\u001b[0;32m   1955\u001b[0m tot_processed_words \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m   1957\u001b[0m \u001b[39mwhile\u001b[39;00m tot_processed_words \u001b[39m<\u001b[39m vocab_size:\n\u001b[1;32m-> 1958\u001b[0m     new_chunk \u001b[39m=\u001b[39m fin\u001b[39m.\u001b[39;49mread(binary_chunk_size)\n\u001b[0;32m   1959\u001b[0m     chunk \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m new_chunk\n\u001b[0;32m   1960\u001b[0m     processed_words, chunk \u001b[39m=\u001b[39m _add_bytes_to_kv(\n\u001b[0;32m   1961\u001b[0m         kv, counts, chunk, vocab_size, vector_size, datatype, unicode_errors, encoding)\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\gzip.py:301\u001b[0m, in \u001b[0;36mGzipFile.read\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    299\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39merrno\u001b[39;00m\n\u001b[0;32m    300\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(errno\u001b[39m.\u001b[39mEBADF, \u001b[39m\"\u001b[39m\u001b[39mread() on write-only GzipFile object\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 301\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buffer\u001b[39m.\u001b[39mread(size)\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\_compression.py:68\u001b[0m, in \u001b[0;36mDecompressReader.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreadinto\u001b[39m(\u001b[39mself\u001b[39m, b):\n\u001b[0;32m     67\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mmemoryview\u001b[39m(b) \u001b[39mas\u001b[39;00m view, view\u001b[39m.\u001b[39mcast(\u001b[39m\"\u001b[39m\u001b[39mB\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m byte_view:\n\u001b[1;32m---> 68\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m(byte_view))\n\u001b[0;32m     69\u001b[0m         byte_view[:\u001b[39mlen\u001b[39m(data)] \u001b[39m=\u001b[39m data\n\u001b[0;32m     70\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlen\u001b[39m(data)\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\gzip.py:507\u001b[0m, in \u001b[0;36m_GzipReader.read\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    504\u001b[0m \u001b[39m# Read a chunk of data from the file\u001b[39;00m\n\u001b[0;32m    505\u001b[0m buf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mread(io\u001b[39m.\u001b[39mDEFAULT_BUFFER_SIZE)\n\u001b[1;32m--> 507\u001b[0m uncompress \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_decompressor\u001b[39m.\u001b[39;49mdecompress(buf, size)\n\u001b[0;32m    508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decompressor\u001b[39m.\u001b[39munconsumed_tail \u001b[39m!=\u001b[39m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    509\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mprepend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decompressor\u001b[39m.\u001b[39munconsumed_tail)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "import re\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "nltk.download('punkt') \n",
    "\n",
    "corpus = pd.read_csv(\"C:/Users/DELL/Downloads/archive (1)/Twitter_Data.csv\")\n",
    "\n",
    "\n",
    "model = api.load(\"word2vec-google-news-300\")  \n",
    "word_vectors = model.wv \n",
    "documents = []  \n",
    "for text in corpus.loc[:5, 'clean_text']:\n",
    "    cleaned_text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    tokens = word_tokenize(cleaned_text)\n",
    "    documents.append(tokens)\n",
    "\n",
    "\n",
    "print(documents)\n",
    "\n",
    "\n",
    "model = Word2Vec(sentences=documents, vector_size=5, window=5, min_count=1, workers=4)\n",
    "model.save(\"word2vec.model\")\n",
    "\n",
    "loaded_model = Word2Vec.load(\"word2vec.model\")\n",
    "\n",
    "\n",
    "vec_doc = [word for doc in documents for word in doc]\n",
    "\n",
    "vecteur = [loaded_model.wv[word] for word in vec_doc]\n",
    "print('representation vectorielle de chaque du document',vecteur)\n",
    "\n",
    "vecteur_moy = np.mean(vecteur , axis=0)\n",
    "\n",
    "print('vecteur moyenne ',vecteur_moy)\n",
    "\n",
    "similarity_scores = cosine_similarity([vecteur_moy], loaded_model.wv.vectors)\n",
    "\n",
    "\n",
    "most_similar_indices = np.argsort(similarity_scores[0])[::-1]\n",
    "most_similar_words = [loaded_model.wv.index_to_key[idx] for idx in most_similar_indices]\n",
    "\n",
    "\n",
    "for word, score in zip(most_similar_words[:5], similarity_scores[0, most_similar_indices][:5]):\n",
    "    print(f\"{word}: {score:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy avec TF-IDF : 0.545\n",
      "Classification Report avec TF-IDF \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.67      0.05      0.09        44\n",
      "         0.0       0.58      0.56      0.57        66\n",
      "         1.0       0.53      0.78      0.63        90\n",
      "\n",
      "    accuracy                           0.55       200\n",
      "   macro avg       0.59      0.46      0.43       200\n",
      "weighted avg       0.57      0.55      0.49       200\n",
      "\n",
      "Accuracy avec Word2Vec : 0.565\n",
      "Classification Report Word2Vec\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.67      0.05      0.09        44\n",
      "         0.0       0.58      0.56      0.57        66\n",
      "         1.0       0.53      0.78      0.63        90\n",
      "\n",
      "    accuracy                           0.55       200\n",
      "   macro avg       0.59      0.46      0.43       200\n",
      "weighted avg       0.57      0.55      0.49       200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy avec (Word2Vec + POS): 0.6\n",
      "Classification Report (Word2Vec + POS) \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.67      0.05      0.09        44\n",
      "         0.0       0.58      0.56      0.57        66\n",
      "         1.0       0.53      0.78      0.63        90\n",
      "\n",
      "    accuracy                           0.55       200\n",
      "   macro avg       0.59      0.46      0.43       200\n",
      "weighted avg       0.57      0.55      0.49       200\n",
      "\n",
      "Accuracy avec (TF-IDF + POS): 0.575\n",
      "Classification Report avec (TF-IDF + POS) \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.67      0.05      0.09        44\n",
      "         0.0       0.58      0.56      0.57        66\n",
      "         1.0       0.53      0.78      0.63        90\n",
      "\n",
      "    accuracy                           0.55       200\n",
      "   macro avg       0.59      0.46      0.43       200\n",
      "weighted avg       0.57      0.55      0.49       200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "import re \n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "\n",
    "\n",
    "# nltk.download('punkt') \n",
    "# nltk.download('wordnet') \n",
    "\n",
    "model = api.load(\"word2vec-google-news-300\")\n",
    "corpus = pd.read_csv(\"C:/Users/DELL/Downloads/archive (1)/Twitter_Data.csv\")\n",
    "\n",
    "corpus_sample = corpus.sample(n=1000, random_state=50)\n",
    "corpus= corpus_sample\n",
    "\n",
    "\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    POS_TAG_MAP = {'N': 'n', 'V': 'v', 'R': 'r', 'J': 'a'}\n",
    "    return POS_TAG_MAP.get(tag[0], 'n') \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def pretraitement(text) :\n",
    " text = re.sub(r'[^\\w\\s]', '', text)\n",
    " tokens = word_tokenize(text.lower())\n",
    " lemmatizer = WordNetLemmatizer()\n",
    " lemmas, pos_tags = zip(*[(lemmatizer.lemmatize(word, get_wordnet_pos(tag)), tag) for word, tag in nltk.pos_tag(tokens)])\n",
    "\n",
    "  \n",
    " \n",
    " return pd.Series([ ' '.join(lemmas), ' '.join(pos_tags) ])\n",
    "\n",
    "corpus[['lemma', 'pos']] = corpus['clean_text'].apply(pretraitement)\n",
    "#print(corpus.head())\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "pos_one_hot = mlb.fit_transform(corpus['pos'].apply(lambda x: x.split()))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Vectorization avec TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus['lemma'])\n",
    "#tfidf_vecteur = tfidf_matrix.toarray()\n",
    "\n",
    "\n",
    " #Vectorization avec Word2Vec\n",
    "word2vec_vectors = []\n",
    "for phrase in corpus['lemma'].str.split():\n",
    "    vectors = [model[word] if word in model else np.zeros(model.vector_size) for word in phrase]\n",
    "    if vectors:\n",
    "      vecteur_moy = np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        average_vector = np.zeros(model.vector_size)\n",
    "    word2vec_vectors.append(vecteur_moy)\n",
    "\n",
    "#---------------------------------------- Partie Entrainement----------------------------------------------------------------\n",
    "\n",
    "# Initialisez le modèle de régression logistique\n",
    "model_reg = LogisticRegression()\n",
    "\n",
    "\n",
    "\n",
    "#TF-IDF \n",
    "\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split( tfidf_matrix, corpus['category'], test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Entraînement du modèle\n",
    "\n",
    "model_reg.fit(X_train1, y_train1)\n",
    "\n",
    "#  prédictions\n",
    "y_pred_tfidf = model_reg.predict(X_test1)\n",
    "\n",
    "# Évaluez les performances\n",
    "\n",
    "accuracy_tfidf = accuracy_score(y_test1, y_pred_tfidf)\n",
    "\n",
    "print(\"Accuracy avec TF-IDF :\", accuracy_tfidf)\n",
    "print(\"Classification Report avec TF-IDF \\n\", classification_report(y_test1, y_pred_tfidf))\n",
    "\n",
    "#Word2Vec\n",
    "\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split( word2vec_vectors, corpus['category'], test_size=0.2, random_state=42)\n",
    "\n",
    "model_reg.fit(X_train3, y_train3)\n",
    "\n",
    "#  prédictions\n",
    "y_pred_word2vec = model_reg.predict(X_test3)\n",
    "\n",
    "# Évaluez les performances\n",
    "accuracy_word2vec = accuracy_score(y_test3, y_pred_word2vec)\n",
    "\n",
    "print(\"Accuracy avec Word2Vec :\", accuracy_word2vec)\n",
    "print(\"Classification Report Word2Vec\\n\", classification_report(y_test1, y_pred_tfidf))\n",
    "\n",
    "\n",
    "#Word2Vec + POS\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.concatenate(( word2vec_vectors, pos_one_hot), axis=1), corpus['category'], test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Entraînement du modèle\n",
    "model_reg.fit(X_train, y_train)\n",
    "\n",
    "#  prédictions\n",
    "y_pred_word2vecpos= model_reg.predict(X_test)\n",
    "\n",
    "# Évaluez les performances\n",
    "accuracy_pos_word2vec = accuracy_score(y_test, y_pred_word2vecpos)\n",
    "\n",
    "print(\"Accuracy avec (Word2Vec + POS):\", accuracy_pos_word2vec)\n",
    "print(\"Classification Report (Word2Vec + POS) \\n\", classification_report(y_test1, y_pred_tfidf))\n",
    "\n",
    "\n",
    "\n",
    "#TF-IDF + POS\n",
    "\n",
    "\n",
    "# Concatenate the TF-IDF matrix and POS one-hot encoded matrix\n",
    "X_combined = hstack([tfidf_matrix, pos_one_hot])\n",
    "\n",
    "# Split the combined matrix into training and testing sets\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X_combined, corpus['category'], test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# Entraînement du modèle\n",
    "model_reg.fit(X_train2, y_train2)\n",
    "\n",
    "#  prédictions\n",
    "y_pred_combined_postfidf = model_reg.predict(X_test2)\n",
    "\n",
    "# Évaluez les performances\n",
    "accuracy_pos_tfidf = accuracy_score(y_test2, y_pred_combined_postfidf)\n",
    "print(\"Accuracy avec (TF-IDF + POS):\", accuracy_pos_tfidf)\n",
    "print(\"Classification Report avec (TF-IDF + POS) \\n\", classification_report(y_test1, y_pred_tfidf))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfidf_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 13\u001b[0m\n\u001b[0;32m      7\u001b[0m model_reg \u001b[38;5;241m=\u001b[39m LogisticRegression()\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#TF-IDF \u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m X_train1, X_test1, y_train1, y_test1 \u001b[38;5;241m=\u001b[39m train_test_split( \u001b[43mtfidf_matrix\u001b[49m, corpus[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m'\u001b[39m], test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Entraînement du modèle\u001b[39;00m\n\u001b[0;32m     17\u001b[0m model_reg\u001b[38;5;241m.\u001b[39mfit(X_train1, y_train1)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tfidf_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Initialisez le modèle de régression logistique\n",
    "model_reg = LogisticRegression()\n",
    "\n",
    "\n",
    "\n",
    "#TF-IDF \n",
    "\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split( tfidf_matrix, corpus['category'], test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Entraînement du modèle\n",
    "model_reg.fit(X_train1, y_train1)\n",
    "\n",
    "#  prédictions\n",
    "y_pred_tfidf = model_reg.predict(X_test1)\n",
    "\n",
    "# Évaluez les performances\n",
    "accuracy_tfidf = accuracy_score(y_test1, y_pred_tfidf)\n",
    "print(\"Accuracy avec TF-IDF :\", accuracy_tfidf)\n",
    "\n",
    "print(\"Classification Report avec TF-IDF \", classification_report(y_test1, y_pred_tfidf))\n",
    "\n",
    "\n",
    "\n",
    "#TF-IDF + POS\n",
    "\n",
    "\n",
    "# Concatenate the TF-IDF matrix and POS one-hot encoded matrix\n",
    "X_combined = hstack([tfidf_matrix, pos_one_hot])\n",
    "\n",
    "# Split the combined matrix into training and testing sets\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X_combined, corpus['category'], test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# Entraînement du modèle\n",
    "model_reg.fit(X_train2, y_train2)\n",
    "\n",
    "#  prédictions\n",
    "y_pred_postfidf = model_reg.predict(X_test2)\n",
    "\n",
    "# Évaluez les performances\n",
    "accuracy_pos_tfidf = accuracy_score(y_test2, y_pred_postfidf)\n",
    "print(\"Accuracy avec (TF-IDF + POS):\", accuracy_pos_tfidf)\n",
    "print(\"Classification Report avec TF-IDF \", classification_report(y_test2, y_pred_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, classification_report\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#TF-IDF + POS\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m X_train2, X_test2, y_train2, y_test2 \u001b[38;5;241m=\u001b[39m train_test_split(\u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mconcatenate(( tfidf_matrix, pos_one_hot), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), corpus[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m'\u001b[39m], test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Entraînement du modèle\u001b[39;00m\n\u001b[0;32m     10\u001b[0m model_reg\u001b[38;5;241m.\u001b[39mfit(X_train2, y_train2)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "#TF-IDF + POS\n",
    "\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(np.concatenate(( tfidf_matrix, pos_one_hot), axis=1), corpus['category'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Entraînement du modèle\n",
    "model_reg.fit(X_train2, y_train2)\n",
    "\n",
    "#  prédictions\n",
    "y_pred_combined_postfidf = model_reg.predict(X_test2)\n",
    "\n",
    "# Évaluez les performances\n",
    "accuracy_pos_tfidf = accuracy_score(y_test2, y_pred_combined_postfidf)\n",
    "print(\"Accuracy avec (TF-IDF + POS):\", accuracy_pos_tfidf)\n",
    "print(\"Classification Report avec TF-IDF + POS \", classification_report(y_test1, y_pred_tfidf))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
